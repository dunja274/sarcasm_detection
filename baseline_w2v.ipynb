{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            label                                           response  \\\n",
       "0         SARCASM  @USER @USER @USER I don't get this .. obviousl...   \n",
       "1         SARCASM  @USER @USER trying to protest about . Talking ...   \n",
       "2         SARCASM  @USER @USER @USER He makes an insane about of ...   \n",
       "3         SARCASM  @USER @USER Meanwhile Trump won't even release...   \n",
       "4         SARCASM  @USER @USER Pretty Sure the Anti-Lincoln Crowd...   \n",
       "...           ...                                                ...   \n",
       "4995  NOT_SARCASM  @USER You don't . I have purchased a lot on Am...   \n",
       "4996  NOT_SARCASM  @USER #Emotions you say ü§î never knew that I th...   \n",
       "4997  NOT_SARCASM  @USER @USER @USER You are so right ... \" Yes !...   \n",
       "4998  NOT_SARCASM  @USER @USER @USER Another lazy delusional vote...   \n",
       "4999  NOT_SARCASM  @USER @USER I hope you know no news outlet fro...   \n",
       "\n",
       "                                                context  \n",
       "0     [A minor child deserves privacy and should be ...  \n",
       "1     [@USER @USER Why is he a loser ? He's just a P...  \n",
       "2     [Donald J . Trump is guilty as charged . The e...  \n",
       "3     [Jamie Raskin tanked Doug Collins . Collins lo...  \n",
       "4     [Man ... y ‚Äô all gone ‚Äú both sides ‚Äù the apoca...  \n",
       "...                                                 ...  \n",
       "4995  [@USER Apologies for the inconvenience you fac...  \n",
       "4996  [@USER ü§î idk tho , I think I ‚Äô m #hungry . But...  \n",
       "4997  [@USER @USER @USER Peace to you , and two coun...  \n",
       "4998  [Bernie Sanders told Elizabeth Warren in priva...  \n",
       "4999  [PDP PROTEST BRAINSTORMING SESSION Deji : We n...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>response</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SARCASM</td>\n      <td>@USER @USER @USER I don't get this .. obviousl...</td>\n      <td>[A minor child deserves privacy and should be ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SARCASM</td>\n      <td>@USER @USER trying to protest about . Talking ...</td>\n      <td>[@USER @USER Why is he a loser ? He's just a P...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SARCASM</td>\n      <td>@USER @USER @USER He makes an insane about of ...</td>\n      <td>[Donald J . Trump is guilty as charged . The e...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>SARCASM</td>\n      <td>@USER @USER Meanwhile Trump won't even release...</td>\n      <td>[Jamie Raskin tanked Doug Collins . Collins lo...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>SARCASM</td>\n      <td>@USER @USER Pretty Sure the Anti-Lincoln Crowd...</td>\n      <td>[Man ... y ‚Äô all gone ‚Äú both sides ‚Äù the apoca...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>NOT_SARCASM</td>\n      <td>@USER You don't . I have purchased a lot on Am...</td>\n      <td>[@USER Apologies for the inconvenience you fac...</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>NOT_SARCASM</td>\n      <td>@USER #Emotions you say ü§î never knew that I th...</td>\n      <td>[@USER ü§î idk tho , I think I ‚Äô m #hungry . But...</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>NOT_SARCASM</td>\n      <td>@USER @USER @USER You are so right ... \" Yes !...</td>\n      <td>[@USER @USER @USER Peace to you , and two coun...</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>NOT_SARCASM</td>\n      <td>@USER @USER @USER Another lazy delusional vote...</td>\n      <td>[Bernie Sanders told Elizabeth Warren in priva...</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>NOT_SARCASM</td>\n      <td>@USER @USER I hope you know no news outlet fro...</td>\n      <td>[PDP PROTEST BRAINSTORMING SESSION Deji : We n...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows √ó 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "twitter = pd.read_json(\"sarcasm_detection_shared_task_twitter_training.jsonl\", lines=True)\r\n",
    "twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            label                                            context  \\\n",
       "0     NOT_SARCASM  [Well now that ‚Äô s problematic AF <URL>, @USER...   \n",
       "1         SARCASM  [Last week the Fake News said that a section o...   \n",
       "2         SARCASM  [@USER Let ‚Äô s Aplaud Brett When he deserves i...   \n",
       "3     NOT_SARCASM  [Women generally hate this president . What's ...   \n",
       "4     NOT_SARCASM  [Dear media Remoaners , you excitedly sharing ...   \n",
       "...           ...                                                ...   \n",
       "1795  NOT_SARCASM  [I have been a business customer of MWeb @USER...   \n",
       "1796      SARCASM  [A woman refuses to have her temperature taken...   \n",
       "1797      SARCASM  [The reason big government wants @USER out is ...   \n",
       "1798  NOT_SARCASM  [Happy #musicmonday and #thanks for #all your ...   \n",
       "1799  NOT_SARCASM  [Not long wrapped on the amazing #January22nd ...   \n",
       "\n",
       "                                               response            id  \n",
       "0     @USER @USER @USER My 3 year old , that just fi...     twitter_1  \n",
       "1     @USER @USER How many verifiable lies has he to...     twitter_2  \n",
       "2     @USER @USER @USER Maybe Docs just a scrub of a...     twitter_3  \n",
       "3     @USER @USER is just a cover up for the real ha...     twitter_4  \n",
       "4     @USER @USER @USER The irony being that he even...     twitter_5  \n",
       "...                                                 ...           ...  \n",
       "1795  @USER @USER @USER is definitely the best out t...  twitter_1796  \n",
       "1796  @USER @USER Ye let her out run wild and infect...  twitter_1797  \n",
       "1797  @USER @USER @USER Thanks for that , I would ha...  twitter_1798  \n",
       "1798  @USER @USER @USER Yes also #found this on #new...  twitter_1799  \n",
       "1799  @USER @USER @USER you still need to send the l...  twitter_1800  \n",
       "\n",
       "[1800 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>context</th>\n      <th>response</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NOT_SARCASM</td>\n      <td>[Well now that ‚Äô s problematic AF &lt;URL&gt;, @USER...</td>\n      <td>@USER @USER @USER My 3 year old , that just fi...</td>\n      <td>twitter_1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SARCASM</td>\n      <td>[Last week the Fake News said that a section o...</td>\n      <td>@USER @USER How many verifiable lies has he to...</td>\n      <td>twitter_2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>SARCASM</td>\n      <td>[@USER Let ‚Äô s Aplaud Brett When he deserves i...</td>\n      <td>@USER @USER @USER Maybe Docs just a scrub of a...</td>\n      <td>twitter_3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NOT_SARCASM</td>\n      <td>[Women generally hate this president . What's ...</td>\n      <td>@USER @USER is just a cover up for the real ha...</td>\n      <td>twitter_4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NOT_SARCASM</td>\n      <td>[Dear media Remoaners , you excitedly sharing ...</td>\n      <td>@USER @USER @USER The irony being that he even...</td>\n      <td>twitter_5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1795</th>\n      <td>NOT_SARCASM</td>\n      <td>[I have been a business customer of MWeb @USER...</td>\n      <td>@USER @USER @USER is definitely the best out t...</td>\n      <td>twitter_1796</td>\n    </tr>\n    <tr>\n      <th>1796</th>\n      <td>SARCASM</td>\n      <td>[A woman refuses to have her temperature taken...</td>\n      <td>@USER @USER Ye let her out run wild and infect...</td>\n      <td>twitter_1797</td>\n    </tr>\n    <tr>\n      <th>1797</th>\n      <td>SARCASM</td>\n      <td>[The reason big government wants @USER out is ...</td>\n      <td>@USER @USER @USER Thanks for that , I would ha...</td>\n      <td>twitter_1798</td>\n    </tr>\n    <tr>\n      <th>1798</th>\n      <td>NOT_SARCASM</td>\n      <td>[Happy #musicmonday and #thanks for #all your ...</td>\n      <td>@USER @USER @USER Yes also #found this on #new...</td>\n      <td>twitter_1799</td>\n    </tr>\n    <tr>\n      <th>1799</th>\n      <td>NOT_SARCASM</td>\n      <td>[Not long wrapped on the amazing #January22nd ...</td>\n      <td>@USER @USER @USER you still need to send the l...</td>\n      <td>twitter_1800</td>\n    </tr>\n  </tbody>\n</table>\n<p>1800 rows √ó 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "twitter_test = pd.read_json(\"sarcasm_detection_shared_task_twitter_testing.jsonl\", lines=True)\n",
    "twitter_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# context_inputs = []\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "all_words = []\n",
    "\n",
    "for sent, c in zip(twitter.response,twitter.context):\n",
    "    tokenized_sentence = nltk.sent_tokenize(sent)   \n",
    "    [all_words.append(tknzr.tokenize(word)) for word in tokenized_sentence]\n",
    "               \n",
    "    for s in c:\n",
    "        tokenized_sentence = nltk.sent_tokenize(s)\n",
    "        [all_words.append(tknzr.tokenize(word)) for word in tokenized_sentence]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent, c in zip(twitter_test.response,twitter_test.context):\n",
    "    tokenized_sentence = nltk.sent_tokenize(sent)   \n",
    "    [all_words.append(tknzr.tokenize(word)) for word in tokenized_sentence]\n",
    "               \n",
    "    for s in c:\n",
    "        tokenized_sentence = nltk.sent_tokenize(s)\n",
    "        [all_words.append(tknzr.tokenize(word)) for word in tokenized_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(all_words, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "input_labels = [ 1 if x=='SARCASM' else 0 for x in twitter.label ]\n",
    "print(len(input_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.0793931 ,  0.48638424, -1.1229906 ,  0.49963397,  0.21646513,\n",
       "       -0.46540236,  0.15479459, -0.2697917 ,  0.2146782 , -1.0751396 ,\n",
       "       -0.7296477 ,  0.2733308 , -0.35501996, -0.6431726 , -0.06901212,\n",
       "       -0.22601998,  0.1519022 ,  0.0943047 ,  0.40569255,  0.08008453,\n",
       "       -0.24464439, -0.67540056,  0.02461376,  1.0961068 ,  0.00250588,\n",
       "       -0.423214  , -0.4256654 , -0.3363572 , -0.31455752, -0.03620967,\n",
       "       -0.66537917,  0.8245871 , -0.9899418 , -0.21840361,  0.14891191,\n",
       "       -0.07799513,  0.39762306, -0.32287696,  0.38427532, -0.08722266,\n",
       "       -1.3546114 ,  0.09626677, -0.31379873, -0.0064878 ,  0.01250277,\n",
       "        0.5879053 ,  0.35641772,  0.2528686 ,  0.1274755 ,  0.8534314 ,\n",
       "       -0.6667619 ,  0.6782415 ,  0.12197105, -0.10737778, -0.378172  ,\n",
       "        0.7372016 , -0.0514598 ,  0.06609046, -1.0480264 ,  0.23337463,\n",
       "       -0.9989966 ,  1.3280729 ,  0.58667326, -0.3528709 , -0.44442666,\n",
       "       -0.8086789 ,  0.10617989, -0.32015726, -0.444193  , -0.32640317,\n",
       "       -0.70629746, -0.22421569, -0.15904813, -0.6258656 , -0.20829166,\n",
       "       -0.3232498 , -0.41767028,  0.5066903 ,  0.17251001,  0.5091804 ,\n",
       "       -0.08117969,  0.24538358,  0.45788702,  0.1378561 , -0.61966205,\n",
       "       -0.12583394,  0.64757466,  0.21214688, -0.08785994,  0.16759934,\n",
       "        0.13079953, -0.264726  ,  0.32105035,  0.8605814 , -0.01596968,\n",
       "       -0.24885072, -1.0521368 , -0.46505985,  0.6800979 ,  0.0336699 ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "word2vec.wv['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenization(response, context):\n",
    "    inputs = []\n",
    "\n",
    "    count = 0\n",
    "    for sent, cont in zip(response,context):\n",
    "        avg1 = 0\n",
    "        avg2 = 0\n",
    "        tokenized_sentence = nltk.sent_tokenize(sent)\n",
    "        for s in tokenized_sentence:\n",
    "            for word in tknzr.tokenize(s):\n",
    "                avg1 += word2vec.wv[word]\n",
    "                \n",
    "        tokenized_sentence = nltk.sent_tokenize(cont[-1])\n",
    "        for s in tokenized_sentence:\n",
    "            for word in tknzr.tokenize(s):\n",
    "                avg2 += word2vec.wv[word]\n",
    "        \n",
    "        avg1 = avg1/len(sent)\n",
    "        avg2 = avg2/len(cont[-1])\n",
    "        \n",
    "        inputs.append(np.concatenate((avg1, avg2)))\n",
    "        \n",
    "    return inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenization(twitter.response, twitter.context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.08618319  0.04059264 -0.23574992  0.1814771   0.10758698 -0.10138808\n -0.03541725 -0.11152712  0.09129095 -0.29180542 -0.09248873  0.05497921\n -0.05929863 -0.18677498 -0.03934625 -0.06811054  0.00588357  0.05355674\n  0.13751052 -0.04101359 -0.03726871 -0.10129276  0.00434243  0.20989485\n -0.02783505 -0.1666221  -0.13475998  0.00998181 -0.0103101   0.1158537\n -0.16813898  0.23089841 -0.15851493  0.0963993  -0.02299205  0.02640633\n  0.17817718 -0.02214679  0.03334793 -0.07418256 -0.18230724  0.06709748\n -0.01762907 -0.00077119 -0.08915568  0.11220354 -0.04628322  0.02173515\n  0.04334205  0.15242578 -0.1136506   0.12672435  0.10350788 -0.05640754\n -0.02227329  0.10689225  0.05695129  0.03215957 -0.17905414  0.03231902\n -0.25548548  0.27907664  0.11208238 -0.05549972 -0.08837143 -0.19040349\n -0.00576095 -0.16980018 -0.04308162 -0.07988128 -0.08136658  0.03325476\n -0.01962367 -0.15680861 -0.12545604  0.08697622 -0.13507216  0.00048474\n  0.03081851  0.08328428 -0.0964598  -0.02565635  0.1715171  -0.0758774\n -0.26219872 -0.07742672  0.19010481  0.04246075 -0.00578806  0.05782422\n  0.08835998 -0.03279537  0.06878836  0.15988842 -0.01367161 -0.08115791\n -0.2334406  -0.1976165   0.12128235  0.1120553   0.0415844   0.01619821\n -0.12917505  0.09443349  0.03476357 -0.06109402 -0.02694687 -0.05654478\n  0.01180008 -0.18127087 -0.05240719  0.05569751  0.02024196 -0.13701826\n -0.02027106 -0.01643395 -0.01448823 -0.00304079  0.0819787   0.02305313\n -0.02206728 -0.06113068 -0.0028803   0.13916522 -0.04043154 -0.09926067\n -0.08462635 -0.04656546  0.03051656  0.07565772 -0.13341565  0.10941568\n -0.08876535  0.05934498 -0.00554334  0.01931361  0.09520648 -0.03155549\n -0.00811104 -0.02415683 -0.12765509  0.02174028 -0.03798545 -0.05094745\n -0.1067293   0.05399268 -0.04467598  0.01647293  0.03378165  0.04810011\n -0.09451742  0.03738464  0.03140856  0.02976164 -0.03992818  0.08338859\n  0.06086398  0.04123887 -0.11580009  0.00945003 -0.14077197  0.16487333\n  0.06589553 -0.02993672 -0.0516523  -0.10772683  0.03740944 -0.13979688\n -0.0357754  -0.07577243 -0.04582827  0.02480964 -0.05002225 -0.05638074\n -0.07713982 -0.02578696 -0.08077391  0.04020999  0.09937513  0.05107764\n -0.03203364  0.00219768  0.06240839 -0.03458489 -0.12832981 -0.02197741\n  0.08451705  0.02974275 -0.00320467  0.02224752  0.0325161  -0.01629281\n -0.02785663  0.11213655  0.00333368 -0.07699965 -0.11374093 -0.13776296\n  0.02951591  0.06006105]\n"
     ]
    }
   ],
   "source": [
    "# print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "test_labels = [ 1 if x=='SARCASM' else 0 for x in twitter_test.label ]\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1800\n"
     ]
    }
   ],
   "source": [
    "tests = tokenization(twitter_test.response, twitter_test.context)\n",
    "print(len(tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(inputs, input_labels, random_state=1383944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3500\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "k: linear c: 1 score: 0.6928571428571428\n",
      "k: linear c: 10 score: 0.694\n",
      "k: linear c: 100 score: 0.696\n",
      "k: linear c: 1000 score: 0.6968571428571428\n",
      "k: rbf c: 1 score: 0.6922857142857144\n",
      "k: rbf c: 10 score: 0.6900000000000001\n",
      "k: rbf c: 100 score: 0.6825714285714286\n",
      "k: rbf c: 1000 score: 0.6502857142857144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "c_values = [1, 10, 100, 1000]\n",
    "kernels = ['linear', 'rbf']\n",
    "#Create a svm \n",
    "for k in kernels:\n",
    "    for c in c_values:\n",
    "        clf = svm.SVC(kernel=k, C=c) # Linear Kernel\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "        print(f'k: {k} c: {c} score: {scores.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Import svm model\n",
    "# from sklearn import svm\n",
    "\n",
    "# #Create a svm Classifier\n",
    "clf = svm.SVC(kernel='rbf', C=10) # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy: 0.5983333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 0.5997745208568207\nRecall: 0.5911111111111111\n0.5983123811087676\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision:\",metrics.precision_score(test_labels, y_pred))\n",
    "precision =metrics.precision_score(test_labels, y_pred)\n",
    "\n",
    "print(\"Recall:\",metrics.recall_score(test_labels, y_pred))\n",
    "recall = metrics.recall_score(test_labels, y_pred)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(test_labels, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}